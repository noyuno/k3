version: "3"
services:
    nginx:
        image: steveltn/https-portal:1
        links:
            - php7
            - gitbucket
            - minio
            - grafana
            - animed
            - portainer
            - owncloud
            - discordbot
            - pe
        ports:
            - 80:80
            - 443:443
        restart: "on-failure:5"
        environment:
            STAGE: production
            DOMAINS: |
                ${DOMAIN},
                ${HOST}.${DOMAIN},
                git.${DOMAIN} -> http://gitbucket:8080,
                s3.${DOMAIN} -> http://minio:9000,
                anime.${DOMAIN},
                status.${DOMAIN} -> http://grafana:3000,
                portainer.${DOMAIN} -> http://portainer:9000,
                photos.${DOMAIN} -> http://photod:80,
                drive.${DOMAIN} -> http://owncloud:8080,
                discord.${DOMAIN} -> http://discordbot:80,
                pi.${DOMAIN} -> http://pe:3000/,
            SERVER_NAMES_HASH_BUCKET_SIZE: 64
            FORCE_RENEW: 'false'
            CLIENT_MAX_BODY_SIZE: '15g'
            WEBSOCKET: "true"
        volumes:
            - ./data/cert:/var/lib/https-portal
            - ./nginx:/var/lib/nginx-conf:ro
            - ./html:/var/www/vhosts
            - ./animed/html:/var/www/vhosts/anime.${DOMAIN}
            - ./data/animed:/var/www/vhosts/anime.${DOMAIN}/data

    php7:
        image: php:7-fpm-alpine
        restart: "on-failure:5"
        volumes:
            - ./nginx/html:/var/www/html

    gitbucket:
        image: gitbucket/gitbucket:latest
        restart: "on-failure:5"
        volumes:
            - ./data/gitbucket/files:/gitbucket
        links:
            - gitbucket-db
        environment:
            GITBUCKET_DB_URL: 'jdbc:postgresql://gitbucket-db/gitbucket'
            GITBUCKET_DB_USER: gitbucket
            GITBUCKET_DB_PASSWORD: ${GITBUCKET_DB_PASSWORD}

    gitbucket-db:
        image: postgres:10.5-alpine
        restart: "on-failure:5"
        environment:
            POSTGRES_USER: gitbucket
            POSTGRES_PASSWORD: ${GITBUCKET_DB_PASSWORD}
        volumes:
            - ./db/gitbucket:/var/lib/postgresql/data

    gitbucket-db-backup:
        build: postgres-backup-s3
        restart: "on-failure:5"
        links:
            - gitbucket-db
            - discordbot
        environment:
            CONTAINER_NAME: gitbucket-db-backup
            S3_REGION: ap-northeast-1
            S3_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
            S3_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
            S3_BUCKET: ${S3_BUCKET}
            S3_PREFIX: backup
            POSTGRES_HOST: gitbucket-db
            POSTGRES_DATABASE: gitbucket
            POSTGRES_USER: gitbucket
            POSTGRES_PASSWORD: ${GITBUCKET_DB_PASSWORD}
            POSTGRES_EXTRA_OPTS: '--schema=public --blobs'
            SCHEDULE: '23 48 17 * * 0,3' # 2:48: sec min hour day month week
            #DATETIME: "true"
            DISCORDBOT: discordbot
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"

    minio:
        image: minio/minio:latest
        restart: "on-failure:5"
        environment:
            MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY}
            MINIO_SECRET_KEY: ${MINIO_SECRET_KEY}
        command: server /data
        volumes:
            - ./data/minio:/data

    owncloud:
        image: owncloud/server:latest
        restart: "on-failure:5"
        links:
            - owncloud-db
        volumes:
            - ./data/owncloud/files:/mnt/data
        environment:
            OWNCLOUD_DOMAIN: drive.${DOMAIN}
            OWNCLOUD_DB_TYPE: pgsql
            OWNCLOUD_DB_HOST: owncloud-db
            OWNCLOUD_DB_NAME: owncloud
            OWNCLOUD_DB_USERNAME: owncloud
            OWNCLOUD_DB_PASSWORD: ${OWNCLOUD_DB_PASSWORD}
            OWNCLOUD_ADMIN_USERNAME: ${OWNCLOUD_ADMIN_USER}
            OWNCLOUD_ADMIN_PASSWORD: ${OWNCLOUD_ADMIN_PASSWORD}
        healthcheck:
            test: ["CMD", "/usr/bin/healthcheck"]
            interval: 30s
            timeout: 10s
            retries: 5
        logging:
            driver: none

    owncloud-db:
        image: postgres:9.6-alpine
        restart: "on-failure:5"
        environment:
            POSTGRES_USER: owncloud
            POSTGRES_PASSWORD: ${OWNCLOUD_DB_PASSWORD}
        volumes:
            - ./db/owncloud:/var/lib/postgresql/data

    owncloud-db-backup:
        build: postgres-backup-s3
        restart: "on-failure:5"
        links:
            - owncloud-db
            - discordbot
        environment:
            CONTAINER_NAME: owncloud-db-backup
            S3_REGION: ap-northeast-1
            S3_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
            S3_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
            S3_BUCKET: ${S3_BUCKET}
            S3_PREFIX: backup
            POSTGRES_HOST: owncloud-db
            POSTGRES_DATABASE: owncloud
            POSTGRES_USER: owncloud
            POSTGRES_PASSWORD: ${OWNCLOUD_DB_PASSWORD}
            POSTGRES_EXTRA_OPTS: '--schema=public --blobs'
            SCHEDULE: '23 16 18 * * 0,3' # 3:16: sec min hour day month week
            #DATETIME: "true"
            DISCORDBOT: discordbot
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"

    backupd:
        build: backupd
        restart: "on-failure:5"
        links:
            - discordbot
        environment:
            NAME: backupd
            AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
            AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
            AWS_REGION: ap-northeast-1
            S3_PATH: s3://${S3_BUCKET}/backup/
            CRON_SCHEDULE: "46 17 * * 0,3" # 2:46
            EXCLUDES: "tmp,db"
            COMPRESS: "true"
            #DATETIME: "true"
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"
        volumes:
            - ./:/data:ro
            - ./backupd:/opt/backupd:ro
            - ./logs/backupd:/logs/backupd
        #command: no-cron

    animed:
        build: animed
        restart: "on-failure:5"
        volumes:
            - ./data/animed:/data

    discordbot:
        build: discordbot
        restart: "on-failure:5"
        expose:
            - "80"
        links:
            #- cadvisor
            - manet
        volumes:
            - ./data/discordbot:/data/discordbot
            - ./logs/discordbot:/logs/discordbot
            - ./discordbot:/opt/discordbot:ro
        environment:
            DEPLOY: /opt/discordbot
            DISCORD_TOKEN: ${DISCORD_TOKEN}
            DISCORD_CHANNEL_NAME: ${DISCORD_CHANNEL_NAME}
            GOOGLE_MAPS_API_KEY: ${GOOGLE_MAPS_API_KEY}
            DARK_SKY_API_KEY: ${DARK_SKY_API_KEY}
            LOCATION: ${LOCATION}
            #CADVISOR: 'cadvisor:8080'
            MANET: 'manet:8891'
            XRAIN_LAT: ${XRAIN_LAT}
            XRAIN_LON: ${XRAIN_LON}
            XRAIN_ZOOM: ${XRAIN_ZOOM}
            #CONTAINERS: 'nginx,php7,gitbucket,gitbucket-db,gitbucket-db-backup,minio,backupd,animed,prometheus,node-exporter,grafana,manet,portainer,photod,rclone-drive-local,rclone-drive-s3,pe'
            MORNING: "22:00" # 07:00
            EVENING: "10:00" # 19:00
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"
            PORT: 80
            
    prometheus:
        image: prom/prometheus
        restart: "on-failure:5"
        volumes:
            - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
        expose:
            - 9090
        links:
            - cadvisor
            - node-exporter
        environment:
            VIRTUAL_HOST: status.${HOST}.${DOMAIN}
            #LETSENCRYPT_HOST: prometheus.prometheus.example.com
            #LETSENCRYPT_EMAIL: hoge@example.com

    cadvisor:
        image: google/cadvisor:latest
        restart: "on-failure:5"
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock:ro
            - /:/rootfs:ro
            - /var/run:/var/run:rw
            - /sys:/sys:ro
            - /var/lib/docker/:/var/lib/docker:ro
            - /dev/disk/:/dev/disk:ro
            - /sys/fs/cgroup:/cgroup:ro
        expose:
            - 8080
        privileged: true

    node-exporter:
        image: prom/node-exporter
        restart: "on-failure:5"
        volumes:
            - /proc:/host/proc:ro
            - /sys:/host/sys:ro
            - /:/rootfs:ro
        expose:
            - 9100

    grafana:
        image: grafana/grafana
        restart: "on-failure:5"
        expose:
            - 3000
        user: "0"
        links:
            - prometheus
            - cadvisor
            - node-exporter
        volumes:
            - ./data/grafana:/var/lib/grafana
        environment:
            GF_SECURITY_ADMIN_USER: ${GRAFANA_USER}
            GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD}
            GF_USERS_ALLOW_SIGN_UP: "false"
            GF_USERS_ALLOW_ORG_CREATE: "false"
            GF_SERVER_ROOT_URL: https://status.${DOMAIN}
            #LETSENCRYPT_HOST: prometheus.example.com
            #LETSENCRYPT_EMAIL: hoge@example.com
        # import -> 395, 22, 893

    manet:
        image: bobey/manet
        expose:
            - "8891"
        command: startup.sh --engine=phantomjs
        restart: "on-failure:5"

    portainer:
        image: portainer/portainer
        command: -H unix:///var/run/docker.sock
        restart: "on-failure:5"
        expose:
            - "9000"
        volumes:
            - /var/run/docker.sock:/var/run/docker.sock
            - ./data/portainer:/data
        environment:
            CAP_HOST_MANAGEMENT: 1

    photod:
        build: photod
        restart: "on-failure:5"
        expose:
            - "80"
        links:
            - discordbot
            - nginx
        environment:
            GOOGLE_OAUTH_CLIENT: ${GOOGLE_OAUTH_CLIENT}
            GOOGLE_OAUTH_SECRET: ${GOOGLE_OAUTH_SECRET}
            BASE_URL: "https://photos.${DOMAIN}"
            DISCORDBOT: "discordbot"
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"
            AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
            AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
            S3_BUCKET: ${S3_BUCKET}
            S3_PREFIX: google/photos
            AWS_REGION: ${AWS_REGION}
            SCHEDULE_TIME: "17:51" # requires zero fill 02:51
            SCHEDULE_WEEKDAY: "0,3"
            ONESHOT: 0
            DEBUG: 0
            ALBUM: 1
            LIBRARY: 1
        volumes:
            - ./data/photod:/data/photod
            - ./logs/photod:/logs/photod
            - ./photod:/opt/photod:ro

    rclone-drive-local:
        build: rcloned
        links:
            - discordbot
        environment:
            RCLONE_CROND_SCHEDULE: "02 18 * * 0,3" # 03:02
            RCLONE_CROND_SOURCE_PATH: "drive:"
            RCLONE_CROND_DESTINATION_PATH: "/data"
            RCLONE_CROND_OPTIONS: "--transfers 1 --buffer-size 1M --use-mmap -v"
            CONTAINER_NAME: "rclone-drive-local"
            DISCORDBOT: discordbot
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"
        cap_add:
            - MKNOD
            - SYS_ADMIN
        restart: "on-failure:5"
        # oneshot
        #command: /opt/rcloned/rclone.sh run sync
        security_opt:
            - apparmor:unconfined
        tty: true
        ulimits:
            nproc: 65535
            nofile:
                soft: 49999
                hard: 99999
        volumes:
            - ./data/rclone-drive:/root/.config/rclone
            - ./tmp/rclone-drive:/data
            - ./rcloned:/opt/rcloned
            - ./logs/rcloned:/logs
            - ./rcloned/monit.d:/etc/monit.d
        # docker run --rm -it -v $(pwd)/data/rclone:/root/.config/rclone openbridge/ob_bulkstash rclone config
        # n s3 4 1 <enter> <key> <secret> 11 <enter> 11 <enter> <enter> <enter> <enter> n y
        # n drive 12 <id> <secret> 2 <enter> <enter> n n <paste> n y
        # q
        logging:
            driver: "json-file" # defaults if not specified
            options:
                max-size: "10m"
                max-file: "3"

    rclone-drive-s3:
        build: rcloned
        links:
            - discordbot
        environment:
            RCLONE_CROND_SCHEDULE: "02 00 * * 0,3" # 09:02
            RCLONE_CROND_SOURCE_PATH: "/data"
            RCLONE_CROND_DESTINATION_PATH: "s3:k3b/google/drive"
            RCLONE_CROND_OPTIONS: "--transfers 1 --buffer-size 1M --use-mmap -v"
            CONTAINER_NAME: "rclone-drive-s3"
            DISCORDBOT: discordbot
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"
        cap_add:
            - MKNOD
            - SYS_ADMIN
        restart: "on-failure:5"
        # oneshot
        #command: /opt/rcloned/rclone.sh run sync
        security_opt:
            - apparmor:unconfined
        tty: true
        ulimits:
            nproc: 65535
            nofile:
                soft: 49999
                hard: 99999
        volumes:
            - ./data/rclone-drive:/root/.config/rclone:ro
            - ./tmp/rclone-drive:/data:ro
            - ./rcloned:/opt/rcloned:ro
            - ./logs/rcloned:/logs
            - ./rcloned/monit.d:/etc/monit.d:ro
        logging:
            driver: "json-file" # defaults if not specified
            options:
                max-size: "10m"
                max-file: "3"
                
    pe:
        build: pe/server
        restart: "on-failure:5"
        expose:
            - "3000"
        links:
            - discordbot
        environment:
            DISCORDBOT: discordbot
            DISCORDBOT_TOKEN: "${DISCORDBOT_TOKEN}"
            PE_TOKEN: "${PE_TOKEN}"
            NODE_ENV: production
        volumes:
            - ./tmp/pe:/data
            #- ./pe/server/routes:/opt/pe/routes:ro
            #- ./pe/server/public:/opt/pe/public
            #- ./pe/server/sass:/opt/pe/sass:ro
            #- ./pe/server/views:/opt/pe/views:ro
            #- ./pe/server/src:/opt/pe/src:ro
